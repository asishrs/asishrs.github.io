[{"content":"","date":"3 January 2025","externalUrl":null,"permalink":"/","section":"Asish Soudhamma","summary":"","title":"Asish Soudhamma","type":"page"},{"content":" Fueling my mind with these amazing reads. ‚ú® ","date":"3 January 2025","externalUrl":null,"permalink":"/books/","section":"Books","summary":"","title":"Books","type":"books"},{"content":" Check out the podcasts I\u0026rsquo;m loving lately! May cause extreme podcast addiction. Proceed with caution. üòâ ","date":"2 January 2025","externalUrl":null,"permalink":"/podcasts/","section":"Podcasts üéß","summary":"","title":"Podcasts üéß","type":"podcasts"},{"content":" Simplify your life with these ‚Äì Feel fee to use the referral links. ","date":"2 January 2025","externalUrl":null,"permalink":"/refer/","section":"Referral Codes ü§ù ","summary":"","title":"Referral Codes ü§ù ","type":"refer"},{"content":"","date":"15 September 2020","externalUrl":null,"permalink":"/tags/blog/","section":"Tags","summary":"","title":"Blog","type":"tags"},{"content":"","date":"15 September 2020","externalUrl":null,"permalink":"/blog/","section":"Blogs","summary":"","title":"Blogs","type":"blog"},{"content":"","date":"15 September 2020","externalUrl":null,"permalink":"/tags/envoy/","section":"Tags","summary":"","title":"Envoy","type":"tags"},{"content":"","date":"15 September 2020","externalUrl":null,"permalink":"/tags/grpc/","section":"Tags","summary":"","title":"GRPC","type":"tags"},{"content":"","date":"15 September 2020","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"15 September 2020","externalUrl":null,"permalink":"/tags/proxyless/","section":"Tags","summary":"","title":"Proxyless","type":"tags"},{"content":" In this post, I will show how you can build a proxy-less load balancing for your gRPC services using the new xDS load balancing.\nYou can find the complete code for this experiment in asishrs/proxyless-grpc-lb\nWhy is load balancing in gRPC difficult? # If you are building gRPC based applications, you may already be aware of the usage of HTTP2 in gRPC. If you are unfamiliar with that, please read below\nhttps://grpc.io/blog/grpc-load-balancing/ https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/ At a high-level, I want you to understand two points.\ngRPC is built on HTTP/2, and HTTP/2 is designed to have a single long-lived TCP connection. To do gRPC load balancing, we need to shift from connection balancing to request balancing. What are the options? # There used to be two options to load balance gRPC requests in a Kubernetes cluster\nHeadless service Using a Proxy (example Envoy, Istio, Linkerd) Recently gRPC announced the support for xDS based load balancing, and as of this time, the gRPC team added support in C-core, Java, and Go languages. This is an essential feature as this will open a third option for load balancing in gRPC, and I will show how to do that in a Kubernetes cluster. gRPC will be moving from its original grpclb protocol to the new xDS protocol.\nxDS API # xDS API is a suite of APIs becoming popular and evolving into a standard used to configure various data plane software.\nIn the xDS API flow, the client uses the following main APIs:\nListener Discovery Service (LDS): Returns Listener resources. Used basically as a convenient root for the gRPC client‚Äôs configuration. Points to the RouteConfiguration. Route Discovery Service (RDS): Returns RouteConfiguration resources. Provides data used to populate the gRPC service config. Points to the Cluster. Cluster Discovery Service (CDS): Returns Cluster resources. Configures things like load balancing policy and load reporting. Points to the ClusterLoadAssignment. Endpoint Discovery Service (EDS): Returns ClusterLoadAssignment resources. Configures the set of endpoints (backend servers) to load balance across and may tell the client to drop requests. gRPC load balancing using xDS API # To leverage xDS load balancing, the gRPC client needs to connect to the xDS server. Clients need to use xds resolver in the target URI used to create the gRPC channel.\nThe diagram below shows the sequence of API calls.\nxDS API calls\nThe xDS server is responsible for discovering the EndPoints for the gRPC server and communicate that to the client. The client then asks for updates in a periodic interval.\nxDS EndPoint Discovery in Kubernetes cluster # In this example, I am using k8s client-go to discover the EndPoints and Envoy go-control-plane as the xDS server. In my implementation, the xDS server polls the Kubernetes endpoints every minute and updates the xDS snapshot with the gRPC server IP address and port. Since I am using a minute interval for polling, clients may take up to a minute to reflect the Endpoint updates.\nNote: Envoy go-control-plane doesn‚Äôt support multiple gRPC client requests. I am using a forked version with a fix for this. I have an issue open to discuss the same with the Envoy team.\nExample Application # I am using two gRPC services (hello and howdy) and clients connecting to those in a load-balanced way in this example application. The below diagram shows the setup.\nSee all in Action # To test the repository quickly, I am going to run all commands via Go Task. You can look into the all commands in the Taskfile.yml (or Taskfile-*.yml) files.\nDeploy the components # # XDS Server # Build xDS Server task xds:build # Deploy xDS Server task xds:deploy # You can run above in a single command task xds:build xds:deploy # gRPC Server # Build gRPC Server task server:build # Deploy gRPC Server task server:deploy # You can run above in a single command task server:build server:deploy # gRPC Client # Build gRPC Client task client:build # Deploy gRPC Client task client:deploy # You can run above in a single command task client:build client:deploy Optional Step ‚Äî Verify all components\nCheck all the components are running.\nRun kubectl get deployments.apps to check all deployments. You must see five deployments in the list.\nNAME READY UP-TO-DATE AVAILABLE AGE hello-client 1/1 1 1 4d23h howdy-client 1/1 1 1 4d23h hello-server 2/2 2 2 4d23h howdy-server 2/2 2 2 4d23h xds-server 1/1 1 1 5d Run kubectl get svc to check all services. You must see four services on the list.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 14d xds-server ClusterIP 10.43.113.60 \u0026lt;none\u0026gt; 18000/TCP 5d hello-server ClusterIP 10.43.78.196 \u0026lt;none\u0026gt; 50051/TCP 4d23h howdy-server ClusterIP 10.43.228.201 \u0026lt;none\u0026gt; 50052/TCP 4d23h Run kubectl get pods to check all pods. You must see seven pods on the list. The pod suffix (after the last -) is going to be different each time.\nNAME READY STATUS RESTARTS AGE howdy-client-659b8f99f5-q6fvx 1/1 Running 1 4d23h hello-client-5f7bdc5f68-2vkfs 1/1 Running 1 4d23h xds-server-86c5f6bfcf-f8wks 1/1 Running 1 5d hello-server-b9ff98f5b-cgvk7 1/1 Running 1 4d23h howdy-server-7d79c584f6-xbv4l 1/1 Running 1 4d23h hello-server-b9ff98f5b-l7dmc 1/1 Running 1 4d23h howdy-server-7d79c584f6-d54zs 1/1 Running 1 4d23h Validate Proxyless (xDS) load balancing # The server responds to the gRPC requests in the code by adding the server host IP. We can use this to identify the target server responding to the client request.\nLet‚Äôs check the logs from hello client by running kubectl logs -f deployment/hello-client\nThere are two essential things in the logs.\nxDS calls\nYou can see logs initiating connections with the xDS server and getting the server‚Äôs xDS protocol response. This is the point where the client-side load balancer picks the available connections.\nExample response\nINFO: 2020/09/07 01:31:28 [xds] [eds-lb 0xc0002ce820] Watch update from xds-client 0xc0001502a0, content: {Drops:[] Localities:[{Endpoints:[{Address:10.42.0.234:50052 HealthStatus:1 Weight:0} {Address:10.42.1.138:50052 HealthStatus:1 Weight:0}] ID:my-region-my-zone- Priority:0 Weight:1000}]} gRPC Server Response\nYou can see the gRPC server responding to the client requests. Using the IP address in the gRPC response, you can see that the response comes from both replicas of hello-grpc-server apps.\nExample response\n{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;client/client.go:51\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Hello Response\u0026#34;,\u0026#34;Response\u0026#34;:\u0026#34;message:\\\u0026#34;Hello gRPC Proxyless LB from host howdy-server-7d79c584f6-xbv4l\\\u0026#34;\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;client/client.go:51\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Hello Response\u0026#34;,\u0026#34;Response\u0026#34;:\u0026#34;message:\\\u0026#34;Hello gRPC Proxyless LB from host howdy-server-7d79c584f6-d54zs\\\u0026#34;\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;client/client.go:51\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Hello Response\u0026#34;,\u0026#34;Response\u0026#34;:\u0026#34;message:\\\u0026#34;Hello gRPC Proxyless LB from host howdy-server-7d79c584f6-xbv4l\\\u0026#34;\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;client/client.go:51\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Hello Response\u0026#34;,\u0026#34;Response\u0026#34;:\u0026#34;message:\\\u0026#34;Hello gRPC Proxyless LB from host howdy-server-7d79c584f6-d54zs\\\u0026#34;\u0026#34;} Similarly, you can check the howdy-client logs kubectl logs -f deployment/howdy-client\nWatch the commands!\nFinal Diagram\nHere is the diagram representing, the whole setup.\nNext Steps # What about scaling the deployments? You can scale (up and down) the gRPC server (both hello and howdy) deployments and see how the xDS load balancing will behave. It may take up to a minute for the client to pick up the changes.\n# Scale hello-server to 5 replicas kubectl scale ‚Äî replicas=5 deployments.apps/hello-server # Scale howdy-server to 5 replicas kubectl scale ‚Äî replicas=5 deployments.apps/howdy-server Bonus Diagram # Here is the diagram with every call in this experiment.\nReferences # https://www.envoyproxy.io/docs/envoy/latest/api-docs/xds_protocol https://github.com/grpc/proposal/blob/master/A27-xds-global-load-balancing.md https://github.com/envoyproxy/data-plane-api/tree/master/envoy/api/v2 https://github.com/grpc/grpc-go/blob/master/examples/features/xds/README.md https://github.com/envoyproxy/go-control-plane/issues/349 https://medium.com/@salmaan.rashid/grpc-xds-loadbalancing-a05f8bd754b8 ","date":"15 September 2020","externalUrl":null,"permalink":"/blog/proxyless-grpc/","section":"Blogs","summary":"Build a proxy-less load balancing for your gRPC services using the new xDS load balancing","title":"Proxyless gRPC load balancing in Kubernetes","type":"blog"},{"content":"","date":"15 September 2020","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"15 September 2020","externalUrl":null,"permalink":"/tags/xds/","section":"Tags","summary":"","title":"XDS","type":"tags"},{"content":" I am a fan of Hugo for building static websites and blogs. It is very easy to set up Hugo and start building webpages using markdown. If you want to add some serious customizations to your pages you can achieve that using shortcodes. You have access to thousands of themes which makes bootstrapping easy.\nIn this article, I am going to show how you can automate the build and deployment of GitHub Pages using Hugo and GitHub Actions. Creating a Hugo website and setting up a GitHub page is out of scope for this article.\nFor building and publishing the website, I am going to use actions-gh-pages.\nThere are two different ways to set up the deployment process.\nUse Single Repository # In this case, you are keeping the Hugo source code and generated Hugo site in the same repository configured for GitHub pages. The Hugo source contains all the files that written (or updated) by you for generating the site and generated Hugo site is the contents inside public directory. In order to do this, you will have a dedicated branch in your repository (example develop ) contains the source and you will be storing the public directory contents in a different branch. Please note, in the case of personal repositories you have to keep the GitHub page files in master branch.\nPros: Less maintenance, as you can manage everything from a single repository.\nCons: This means you will have to manage entirely different contents between your source branch and the GitHub pages branch.\nUse Multiple Repositories # In this case, you are keeping the Hugo source in a dedicated repository and generated Hugo site contents inside public directory in a different repository configured with GitHub pages.\nPros: Much cleaner approach and align well with the git branching strategies.\nCons: Additional work to set up and configure two repositories.\nI am going to use the multiple repositories set up with personal GitHub pages in this article.\nPrerequisites: # Create and configure a repository for GitHub page Create an additional repository for Hugo source files. Build a sample Hugo site. For testing the set up you can use a sample Hugo site like this (check the exampleSite ) Test your Hugo site using hugo server -D Configure GitHub Actions # You need to set up keys on the above repositories to enable publish.\nGenerate your deploy key with the following command. ssh-keygen -t rsa -b 4096 -C \u0026#34;$(git config user.email)\u0026#34; -f master -N \u0026#34;\u0026#34; \\# You will get 2 files: \\# master.pub (public key) \\# master (private key) 2. Add your private key as a secret with name ACTIONS_DEPLOY_KEY in the Hugo Source Repository.\nNavigate to Your Hugo Source Repository \u0026gt; Settings \u0026gt; Secrets \u0026gt; Add new secrets and the name the secret as ACTIONS_DEPLOY_KEY and paste the contents of master file in the value.\n3. Add your public key as deployment key in the GitHub pages repository\nNavigate to Your GitHub pages Repository \u0026gt; Settings \u0026gt; Deploy Key\u0026gt; Add new deploy key and set title something you can relate to the private key (like Public Key for my site deploy) and paste the contents of master.pub file in the value.\n4. Add below file .github/workflows/pages.yml location of your Hugo source code and replace username/external-repository with your GitHub username and GitHub pages repository name.\n5. Commit your Hugo source code repository changes and push to GitHub. If you are using any branch other than master , you need to merge the changes to master for the workflow to start. You can change that setting in on:push:branches part of the workflow file.\n6. Check the workflow in the Actions tab of your Hugo Source repository.\nOnce your workflow completed, you can see new commits on the GitHub pages repository master branch and in few minutes the changes will appear on your GitHub page URL.\n","date":"4 February 2020","externalUrl":null,"permalink":"/blog/hugo-ghpages/","section":"Blogs","summary":"Explore how you can automate the build and deployment of GitHub Pages using Hugo and \u003ca href=\"https://github.com/features/actions\" target=\"_blank\"\u003eGitHub Actions\u003c/a\u003e","title":"Automate your GitHub Pages Deployment using Hugo and Actions","type":"blog"},{"content":"","date":"4 February 2020","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":"","date":"4 February 2020","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"4 February 2020","externalUrl":null,"permalink":"/categories/documentation/","section":"Categories","summary":"","title":"Documentation","type":"categories"},{"content":"","date":"4 February 2020","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"Github","type":"tags"},{"content":"","date":"4 February 2020","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"1 February 2020","externalUrl":null,"permalink":"/tags/apigateway/","section":"Tags","summary":"","title":"Apigateway","type":"tags"},{"content":" In this article, I will show how you can set up a protected API endpoint using AWS Lamda, API Gateway, and automate the deployment of the stack using AWS CloudFormation.\nIf you don‚Äôt have prior knowledge of the resources/components mentioned above, read the high-level description below.\nAWS Lambda # AWS Lambda lets you run code without provisioning or managing servers. With Lambda, you can run code for virtually any type of application or backend service ‚Äî all with zero administration. Read more\nAmazon API Gateway # Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the ‚Äúfront door‚Äù for applications to access data, business logic, or functionality from your backend services. Read more\nAWS CloudFormation # AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. Read more\nIn the example, I have a sample inline API service deployed as Lambda and then expose the endpoint from that via the API gateway. For protecting the APIs, I will be using a managed API Key provided by AWS. There are many ways to protect an API endpoint depending on different use cases; I am not going to cover those in this article.\nCloudFormation Template # Below is the template file, we will be using.\nLet‚Äôs take a look at the important definitions.\nLambda function # I am using an inline lambda function based on NodeJS. The function responds with `Hello World!` for the API invocation.\nAPI Key # As mentioned before, I am using an API Key to protect the API. Below code, block define an API key, Usage Plan, and Usage Plan Key. You can see that I am adding ApiKeyRequired: true as part of the AWS::ApiGateway::Method\nAPI Gateway # Defining the Gateway includes many resources, including API stage and Deployment.\nIn addition to this, you can also see a couple of IAM groups (AWS::IAM::Role) are defined on the template. One group is for the Gateway to access the Lambda function and another one for Cloud Watch log creation.\nI also defined a few parameters; these are optional.\nCreate the Stack # Now that you understand the resource definitions on the template file, we can proceed to create the stack. I am going to create the stack using `aws cli`. Refer this to setup your `aws cli`. You can also create the stack via AWS Console\nValidate the template file # aws cloudformation validate-template ‚Äî template-body file://stack.yaml\nDeploy the stack # aws cloudformation deploy ‚Äî stack-name lambda-api ‚Äî template-file stack.yaml ‚Äî capabilities CAPABILITY\\_IAM Once the stack is created, you will below in your console\nWaiting for changeset to be created.. Waiting for stack create/update to complete Successfully created/updated stack ‚Äî lambda-api List the APIs # aws apigateway get-rest-apis\nGet the API Id from above and construct your api url using the format https://{restapi_id}.execute-api.{region}.amazonaws.com/{stage_name}/\nIf you are unsure about the region, you can run aws configure get region\nGet your API Key # Execute below command and copy value\naws apigateway get-api-keys ‚Äî include-value\nInvoke the API # curl -X GET \\\\ https:/{restapi\\_id}.execute-api.{region}.amazonaws.com/v1/hello \\\\ -H ‚Äòx-api-key: {your api key}‚Äô For successful requests, you can see a response like below.\nHello World!\nConclusion # You now have a protected API with a Lambda integration! As a next step, you can replace the inline Lambda function with a real-world Lambda code. You can upload your Lambda functions to the Amazon S3 bucket and use that in your template. Check the S3 example here . Good luck!\n","date":"1 February 2020","externalUrl":null,"permalink":"/blog/aws-lambda-apigw-cf/","section":"Blogs","summary":"Set up a protected API endpoint using AWS Lamda, API Gateway, and automate the deployment of the stack using AWS CloudFormation.","title":"Automated and Authenticated APIs stack using Lambda, API Gateway and Cloudformation (AWS)","type":"blog"},{"content":"","date":"1 February 2020","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"Aws","type":"tags"},{"content":"","date":"1 February 2020","externalUrl":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"Cloud","type":"tags"},{"content":"","date":"1 February 2020","externalUrl":null,"permalink":"/tags/cloudformation/","section":"Tags","summary":"","title":"Cloudformation","type":"tags"},{"content":"","date":"1 February 2020","externalUrl":null,"permalink":"/tags/lambda/","section":"Tags","summary":"","title":"Lambda","type":"tags"},{"content":"","date":"17 October 2018","externalUrl":null,"permalink":"/tags/kafka/","section":"Tags","summary":"","title":"Kafka","type":"tags"},{"content":"","date":"17 October 2018","externalUrl":null,"permalink":"/tags/spring/","section":"Tags","summary":"","title":"Spring","type":"tags"},{"content":" Spring Cloud Config is one of the best features that Spring provides as part of the framework. Spring Cloud Config allows your java application to follow Externalized configuration pattern which is must have if you are building microservices. Additionally, you can also enable the automatic config refresh in Spring Cloud Config so that all your components receive the latest configuration values when there is a change in the configuration. For automatic configuration delivery, Spring Cloud use Kafka or RabbitMQ messaging platforms. This article is going to explain how to define your own Kafka topics with Spring Cloud Config.\nUsing default configuration, Spring Cloud Config use predefined Kafka channel springCloudBusInput and springCloudBusOutput. Also, Kafka configuration expects you to provide the zookeeper nodes using the option spring.cloud.stream.kafka.binder.zkNodes. This works well if you are using a Kafka broker that you manage or you have control. In that instance, your configuration looks like below.\nspring: cloud: stream: kafka: binder: zkNodes: \u0026#34;\u0026lt;zookeeper-host\u0026gt;:\u0026lt;zookeeper-port\u0026gt;\u0026#34; brokers: \u0026#34;\u0026lt;broker-host\u0026gt;:\u0026lt;broker-port\u0026gt;\u0026#34; What if you have to mention your own Kafka topics? # Above configuration won‚Äôt work if you are using a shared Kafka infrastructure which you cannot control, or you cannot create a topic dynamically on the Kafka. Luckily spring provides configuration options for you to pass custom Kafka topics and disable the auto topic creation.\nBelow are the options you can use for supply your topic names.\nspring.cloud.stream.bindings.springCloudBusInput=\u0026lt;your\\_custom\\_input\\_topic\\_name\u0026gt; spring.cloud.stream.bindings.springCloudBusOutput=\u0026lt;your\\_custom\\_output\\_topic\\_name\u0026gt; You can disable the automatic topic creation using below option.\nspring.cloud.stream.kafka.binder.autoCreateTopics=false Complete configuration looks like below\nspring.cloud.stream: bindings: springCloudBusInput: destination: \u0026lt;your\\_custom\\_kafka\\_input\\_topic\u0026gt; group: \u0026lt;optional group id\u0026gt;springCloudBusOutput: destination: \u0026lt;your\\_custom\\_kafka\\_output\\_topic\u0026gt; kafka.binder: autoCreateTopics: false brokers: \u0026lt;your\\_kafka\\_broker\\_host\u0026gt;:\u0026lt;your\\_kafka\\_broker\\_port\u0026gt; Bonus ‚Äî How can you use this with mutual authentication? # Spring Cloud Stream supports configuration options for mutual authentication via SSL. Here is a sample configuration.\nspring.cloud.stream: kafka.binder: autoCreateTopics: false brokers: \u0026lt;your\\_kafka\\_broker\\_host\u0026gt;:\u0026lt;your\\_kafka\\_broker\\_port\u0026gt; configuration: ‚Äú\\[security.protocol\\]‚Äú: SSL ‚Äú\\[ssl.enabled\\]‚Äú: true ‚Äú\\[ssl.truststore.location\\]‚Äú: \u0026lt;path/truststore\\_file\u0026gt; ‚Äú\\[ssl.truststore.password\\]‚Äú: \u0026lt;your\\_truststore\\_password\u0026gt; ‚Äú\\[ssl.keystore.location\\]‚Äú: \u0026lt;path/keystore\\_file\u0026gt; ‚Äú\\[ssl.keystore.password\\]‚Äú: \u0026lt;your\\_keystore\\_password\u0026gt; ‚Äú\\[ssl.key.password\\]‚Äú: \u0026lt;your\\_key\\_password\u0026gt; ‚Äú\\[sasl.mechanism\\]‚Äú: PLAIN ‚Äú\\[ssl.protocol\\]‚Äú: TLSv1.2 // Check this with the Cert ‚Äú\\[ssl.enabled.protocols\\]‚Äú: TLSv1.2 // Check this with the Cert ‚Äú\\[ssl.endpoint.identification.algorithm\\]‚Äú: HTTPS Reference # Spring Cloud Config Spring Cloud ","date":"17 October 2018","externalUrl":null,"permalink":"/blog/spring-cloud-config-kafka/","section":"Blogs","summary":"Spring Cloud Config allows your java application to follow Externalized configuration pattern which is must have if you are building microservices. Explore how to leverage that to define your own Kafka topics","title":"Spring Cloud Config Auto Refresh + Custom Kafka topic","type":"blog"},{"content":"","date":"17 October 2018","externalUrl":null,"permalink":"/tags/cgroup/","section":"Tags","summary":"","title":"Cgroup","type":"tags"},{"content":"","date":"17 October 2018","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":" One of the things you have to keep in mind in the cloud journey is limiting the resources (CPU and memory) for all running containers. Restrict CPU and Memory utilization across all containers is very important especially if you are running multiple containers. Docker presents an option called cgroup-parent for this purpose. You can check the official docker documentation on cgroup-parent here. Unfortunately, docker documentation doesn‚Äôt provide enough details for you start using use cgroup-parent. You can continue to read this post if you are still looking a way to apply resource limitation to your containers.\nFirst, What is cgroups (control groups)? # cgroups is a feature provided by Linux kernel which allows a user to set limits on resource usage ‚Äî cpu, memory, network, disk i/o. In another way, this allows specifying process level resource limitations in Linux. Read more about cgroups in redhat docs.\nCan I use docker \u0026ndash;memory, \u0026ndash;cpus options? Yes, only if you have one container per host! # Yes, you can use the memory and cpu limit options mention mentioned on docker documentation. One of the things you need to keep in mind these options are per containers. For example, if you have two core cpu and you start two containers with option --cpus=\u0026quot;1\u0026quot;, your containers are allowed to consume 100% of cpu (2 cores). You can run two containers with one cpu each using below commands.\n‚áí docker run -it --rm --cpus=\u0026#34;1\u0026#34; busybox ‚áí docker run -it --rm --cpus=\u0026#34;1\u0026#34; busybox Once these containers are up, check the cpu allocated in cgroups. You will see that docker set the cpu limit you mentioned at the container start but per container.\n/ # cat /sys/fs/cgroup/cpu/cpu.cfs\\_quota\\_us üòïSo, how do we apply the restrictions consistently?\nHere comes the savior cgroup-parent # For you to use the cgroup-parent option, first you have to define a cgroup on the host with desired limits. Different host os provides packages for creating cgroups, in this article I am going to explain how to use that using centos. I am going to do this in three steps (1) create cgroup on host for limiting cpu and memory (2) adding the cgroup-parent to the containers (3) stress test the containers.\nTo make this easier, I have created a vagrant box with centos7 as the host and use an alpine image with stress for the containers. I also added htop on the centos to check the resource usage. You can clone repo and use vagrant up --provision to provision the box.\n‚áí git clone [https://github.com/asishrs/docker-cgroup-parent.git](https://github.com/asishrs/docker-cgroup-parent.git) \u0026amp;\u0026amp; docker-cgroup-parent ‚áí vagrant up --provision Create cgroup on host for limiting cpu and memory # I am using cgcreate command for creating a new cgroup with name c_group-limit._\ncgcreate -g cpu:cgroup-limit #Next, I am going to set cpu usage by 50% echo 100000 \u0026gt; /sys/fs/cgroup/cpu/cgroup-limit/cpu.cfs\\_quota\\_us echo 100000 \u0026gt; /sys/fs/cgroup/cpu/cgroup-limit/cpu.cfs\\_period\\_us You need to create one cgroup per resource type, in this case, I am trying to limit the cpu hence cpu: in my command. You can check more about cpu.cfs_quota_us and cpu.cfs_period_us here.\nNext, I am going to create a cgroup to limit memory to _100mb (_104857600 bytes).\ncgcreate -g memory:cgroup-limit echo 104857600 \u0026gt; /sys/fs/cgroup/memory/cgroup-limit/memory.limit\\_in\\_bytes üëÅÔ∏èCheck memory: in the command and the name cgroup-limit. Since I want to use a single cgroup to control the cpu and memory, I am using the same name for both cgroups.\nYou can check more about memory limits here.\nAdding the cgroup-parent to the containers # You can add cgroup-parent to docker in a few different ways. The goal here is to tell all containers running on the daemon to use the same resource limits. 1. Use **--cgroup-parent** option You can pass option --cgroup-parent during the container creation.\ndocker run -it --rm --cgroup-parent=/climit-cgroup/ \u0026lt;\u0026lt;image-name\u0026gt;\u0026gt; 2. Use **cgroup_parent** option in compose file You can specify cgroup as an optional parameter for the container in the docker compose. ‚ö†Ô∏èAccording to docker compose docs, this option is ignored when deploying a stack in swarm mode with a (version 3) Compose file. 3. Use **--cgroup-parent** option on dockerd You can use the --cgroup-parent option on dockerd command or specify this in the config.json\nI am using option #1--cgroup-parent at container creation for testing this.\nStress test the containers # To verify the settings, I am going to use stress which allows to reserve cpu and memory so that I can see how the resource limits are working at peak.\nHere is the htop output before starting any containers. CPU is 0% and memory is at 115mb, we will check this once we start the containers with and without cgroups.\nTest the containers without cgroup limit.\nI am going to run this from the vagrant host as I have a specific image (asishrs/alpine-stress) I can use for stress testing.\n‚áí vagrant up --provision // (Waiting...) This might take a while. ‚áí vagrant ssh // Now we are inside the vagrant box. // Here I am creating two conatiners without cgroup parent. \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -d asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -d asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s As you notice, I have started stress as part of the container. We are asking stress to use 2 cpus -c2, one IO -i 1, one worker -m 1, allocate 300M memory--vm-bytes 300Mand run for 60 seconds -t 60s. You can learn all options by adding --verbose at the end of stress command.\nHere is the screenshot from htop. You can see that the cpu utilization is nearly 100% and the memory is at 607mb.\nTest the containers with per container cgroup limit.\n// Here I am creating two conatiners with per container limits \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm --cpus=\u0026#34;1\u0026#34; --memory=\u0026#34;300m\u0026#34; -d asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -cpus=\u0026#34;1\u0026#34; --memory=\u0026#34;300m\u0026#34; -d asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s I have used options --cpus=\u0026quot;1\u0026quot;and --memory=\u0026quot;300m\u0026quot; during the container creation but this is applying the limit per container, that means both of the containers collectively going to consume 100% of cpu and 600mb memory.\nHere is the output of htop. You can see that the cpu utilization is nearly 100% and the memory is at 721mb.\nNow run the container with **--cgroup-parent**.\n// Here I am creating two conatiners with cgroup-parent. \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -d --cgroup-parent=/cgroup-limit/ asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -d --cgroup-parent=/cgroup-limit/ asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s In the above command, I am passing the option‚Äî-cgroup-parent=/cgroup-limit/and that will restrict the overall all CPU utilization to 1 cpu and memory to 100mb.\nHere is the screenshot from htop.\nüò≤ You can see that the cpu utilization is nearly 50% and the memory is at 223mb (this includes the memory for other processes on the host).\nNext Steps: Chekc how can you persist the cgroups so that can create atomic hosts.\nReference # https://docs.docker.com/engine/reference/commandline/dockerd/#miscellaneous-options https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01 ","date":"17 October 2018","externalUrl":null,"permalink":"/blog/docker-resource-limit/","section":"Blogs","summary":"Leverage Docker cgroup-parent to efficiently manage CPU and Memory utilization across all containers","title":"Docker limit resource utilization using cgroup-parent","type":"blog"},{"content":" I‚Äôm a creative and self-motivated problem solver, who like to make solutions with the help of technologies. I can take complex ideas and problems and conceptualize in a way that my audience at all levels can digest those very quickly. I thrive in collaborative functions and have the unique ability to identify and strategize opportunities. Recent Proxyless gRPC load balancing in Kubernetes 15 September 2020\u0026middot;1138 words\u0026middot;6 mins Build a proxy-less load balancing for your gRPC services using the new xDS load balancing Automate your GitHub Pages Deployment using Hugo and Actions 4 February 2020\u0026middot;643 words\u0026middot;4 mins Explore how you can automate the build and deployment of GitHub Pages using Hugo and GitHub Actions Automated and Authenticated APIs stack using Lambda, API Gateway and Cloudformation (AWS) 1 February 2020\u0026middot;638 words\u0026middot;3 mins Set up a protected API endpoint using AWS Lamda, API Gateway, and automate the deployment of the stack using AWS CloudFormation. ","externalUrl":null,"permalink":"/about/","section":"Asish Soudhamma","summary":"","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":" Some of my favorite photos. Check out my full portfolio on 500px ","externalUrl":null,"permalink":"/photos/","section":"Asish Soudhamma","summary":"","title":"Photos","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]