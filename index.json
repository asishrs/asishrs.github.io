[{"categories":["Documentation"],"content":" In this post, I will show how you can build a proxy-less load balancing for your gRPC services using the new xDS load balancing. You can find the complete code for this experiment in asishrs/proxyless-grpc-lb Why is load balancing in gRPC difficult? If you are building gRPC based applications, you may already be aware of the usage of HTTP2 in gRPC. If you are unfamiliar with that, please read below https://grpc.io/blog/grpc-load-balancing/ https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/ At a high-level, I want you to understand two points. gRPC is built on HTTP/2, and HTTP/2 is designed to have a single long-lived TCP connection. To do gRPC load balancing, we need to shift from connection balancing to request balancing. What are the options? There used to be two options to load balance gRPC requests in a Kubernetes cluster Headless service Using a Proxy (example Envoy, Istio, Linkerd) Recently gRPC announced the support for xDS based load balancing, and as of this time, the gRPC team added support in C-core, Java, and Go languages. This is an essential feature as this will open a third option for load balancing in gRPC, and I will show how to do that in a Kubernetes cluster. gRPC will be moving from its original grpclb protocol to the new xDS protocol. xDS API xDS API is a suite of APIs becoming popular and evolving into a standard used to configure various data plane software. In the xDS API flow, the client uses the following main APIs: Listener Discovery Service (LDS): Returns Listener resources. Used basically as a convenient root for the gRPC client’s configuration. Points to the RouteConfiguration. Route Discovery Service (RDS): Returns RouteConfiguration resources. Provides data used to populate the gRPC service config. Points to the Cluster. Cluster Discovery Service (CDS): Returns Cluster resources. Configures things like load balancing policy and load reporting. Points to the ClusterLoadAssignment. Endpoint Discovery Service (EDS): Returns ClusterLoadAssignment resources. Configures the set of endpoints (backend servers) to load balance across and may tell the client to drop requests. gRPC load balancing using xDS API To leverage xDS load balancing, the gRPC client needs to connect to the xDS server. Clients need to use xds resolver in the target URI used to create the gRPC channel. The diagram below shows the sequence of API calls. xDS API calls The xDS server is responsible for discovering the EndPoints for the gRPC server and communicate that to the client. The client then asks for updates in a periodic interval. xDS EndPoint Discovery in Kubernetes cluster In this example, I am using k8s client-go to discover the EndPoints and Envoy go-control-plane as the xDS server. In my implementation, the xDS server polls the Kubernetes endpoints every minute and updates the xDS snapshot with the gRPC server IP address and port. Since I am using a minute interval for polling, clients may take up to a minute to reflect the Endpoint updates. Note: Envoy go-control-plane doesn’t support multiple gRPC client requests. I am using a forked version with a fix for this. I have an issue open to discuss the same with the Envoy team. Example Application I am using two gRPC services (hello and howdy) and clients connecting to those in a load-balanced way in this example application. The below diagram shows the setup. See all in Action To test the repository quickly, I am going to run all commands via Go Task. You can look into the all commands in the Taskfile.yml (or Taskfile-*.yml) files. ","date":"2020-09-15","objectID":"/2020/09/proxyless-grpc/:0:0","tags":["blog","gRPC","proxyless","kubernetes","Envoy","xDS"],"title":"Proxyless gRPC load balancing in Kubernetes","uri":"/2020/09/proxyless-grpc/"},{"categories":["Documentation"],"content":"Deploy the components # XDS Server # Build xDS Server task xds:build # Deploy xDS Server task xds:deploy # You can run above in a single command task xds:build xds:deploy # gRPC Server # Build gRPC Server task server:build # Deploy gRPC Server task server:deploy # You can run above in a single command task server:build server:deploy # gRPC Client # Build gRPC Client task client:build # Deploy gRPC Client task client:deploy # You can run above in a single command task client:build client:deploy Optional Step — Verify all components Check all the components are running. Run kubectl get deployments.apps to check all deployments. You must see five deployments in the list. NAME READY UP-TO-DATE AVAILABLE AGE hello-client 1/1 1 1 4d23h howdy-client 1/1 1 1 4d23h hello-server 2/2 2 2 4d23h howdy-server 2/2 2 2 4d23h xds-server 1/1 1 1 5d Run kubectl get svc to check all services. You must see four services on the list. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u003cnone\u003e 443/TCP 14d xds-server ClusterIP 10.43.113.60 \u003cnone\u003e 18000/TCP 5d hello-server ClusterIP 10.43.78.196 \u003cnone\u003e 50051/TCP 4d23h howdy-server ClusterIP 10.43.228.201 \u003cnone\u003e 50052/TCP 4d23h Run kubectl get pods to check all pods. You must see seven pods on the list. The pod suffix (after the last -) is going to be different each time. NAME READY STATUS RESTARTS AGE howdy-client-659b8f99f5-q6fvx 1/1 Running 1 4d23h hello-client-5f7bdc5f68-2vkfs 1/1 Running 1 4d23h xds-server-86c5f6bfcf-f8wks 1/1 Running 1 5d hello-server-b9ff98f5b-cgvk7 1/1 Running 1 4d23h howdy-server-7d79c584f6-xbv4l 1/1 Running 1 4d23h hello-server-b9ff98f5b-l7dmc 1/1 Running 1 4d23h howdy-server-7d79c584f6-d54zs 1/1 Running 1 4d23h Validate Proxyless (xDS) load balancing The server responds to the gRPC requests in the code by adding the server host IP. We can use this to identify the target server responding to the client request. Let’s check the logs from hello client by running kubectl logs -f deployment/hello-client There are two essential things in the logs. xDS calls You can see logs initiating connections with the xDS server and getting the server’s xDS protocol response. This is the point where the client-side load balancer picks the available connections. Example response INFO: 2020/09/07 01:31:28 [xds] [eds-lb 0xc0002ce820] Watch update from xds-client 0xc0001502a0, content: {Drops:[] Localities:[{Endpoints:[{Address:10.42.0.234:50052 HealthStatus:1 Weight:0} {Address:10.42.1.138:50052 HealthStatus:1 Weight:0}] ID:my-region-my-zone- Priority:0 Weight:1000}]} gRPC Server Response You can see the gRPC server responding to the client requests. Using the IP address in the gRPC response, you can see that the response comes from both replicas of hello-grpc-server apps. Example response {\"level\":\"info\",\"caller\":\"client/client.go:51\",\"msg\":\"Hello Response\",\"Response\":\"message:\\\"Hello gRPC Proxyless LB from host howdy-server-7d79c584f6-xbv4l\\\"\"} {\"level\":\"info\",\"caller\":\"client/client.go:51\",\"msg\":\"Hello Response\",\"Response\":\"message:\\\"Hello gRPC Proxyless LB from host howdy-server-7d79c584f6-d54zs\\\"\"} {\"level\":\"info\",\"caller\":\"client/client.go:51\",\"msg\":\"Hello Response\",\"Response\":\"message:\\\"Hello gRPC Proxyless LB from host howdy-server-7d79c584f6-xbv4l\\\"\"} {\"level\":\"info\",\"caller\":\"client/client.go:51\",\"msg\":\"Hello Response\",\"Response\":\"message:\\\"Hello gRPC Proxyless LB from host howdy-server-7d79c584f6-d54zs\\\"\"} Similarly, you can check the howdy-client logs kubectl logs -f deployment/howdy-client Watch the commands! Final Diagram Here is the diagram representing, the whole setup. Next Steps What about scaling the deployments? You can scale (up and down) the gRPC server (both hello and howdy) deployments and see how the xDS load balancing will behave. It may take up to a minute for the client to pick up the changes. \\# Scale hello-server to 5 replicas kubectl scale — replicas=5 deployments.apps/hello-server\\# Scale howdy-server to 5 replicas kubectl scale — replicas=5 ","date":"2020-09-15","objectID":"/2020/09/proxyless-grpc/:1:0","tags":["blog","gRPC","proxyless","kubernetes","Envoy","xDS"],"title":"Proxyless gRPC load balancing in Kubernetes","uri":"/2020/09/proxyless-grpc/"},{"categories":["Documentation"],"content":" I am a fan of Hugo for building static websites and blogs. It is very easy to set up Hugo and start building webpages using markdown. If you want to add some serious customizations to your pages you can achieve that using shortcodes. You have access to thousands of themes which makes bootstrapping easy. In this article, I am going to show how you can automate the build and deployment of GitHub Pages using Hugo and GitHub Actions. Creating a Hugo website and setting up a GitHub page is out of scope for this article. For building and publishing the website, I am going to use actions-gh-pages. There are two different ways to set up the deployment process. ","date":"2020-02-04","objectID":"/2020/02/hugo-githubactions/:0:0","tags":["hugo","blog","github","automation"],"title":"Automate your GitHub Pages Deployment using Hugo and Actions","uri":"/2020/02/hugo-githubactions/"},{"categories":["Documentation"],"content":"Use Single Repository In this case, you are keeping the Hugo source code and generated Hugo site in the same repository configured for GitHub pages. The Hugo source contains all the files that written (or updated) by you for generating the site and generated Hugo site is the contents inside public directory. In order to do this, you will have a dedicated branch in your repository (example develop ) contains the source and you will be storing the public directory contents in a different branch. Please note, in the case of personal repositories you have to keep the GitHub page files in master branch. Pros: Less maintenance, as you can manage everything from a single repository. Cons: This means you will have to manage entirely different contents between your source branch and the GitHub pages branch. ","date":"2020-02-04","objectID":"/2020/02/hugo-githubactions/:1:0","tags":["hugo","blog","github","automation"],"title":"Automate your GitHub Pages Deployment using Hugo and Actions","uri":"/2020/02/hugo-githubactions/"},{"categories":["Documentation"],"content":"Use Multiple Repositories In this case, you are keeping the Hugo source in a dedicated repository and generated Hugo site contents inside public directory in a different repository configured with GitHub pages. Pros: Much cleaner approach and align well with the git branching strategies. Cons: Additional work to set up and configure two repositories. I am going to use the multiple repositories set up with personal GitHub pages in this article. ","date":"2020-02-04","objectID":"/2020/02/hugo-githubactions/:2:0","tags":["hugo","blog","github","automation"],"title":"Automate your GitHub Pages Deployment using Hugo and Actions","uri":"/2020/02/hugo-githubactions/"},{"categories":["Documentation"],"content":"Prerequisites: Create and configure a repository for GitHub page Create an additional repository for Hugo source files. Build a sample Hugo site. For testing the set up you can use a sample Hugo site like this (check the exampleSite ) Test your Hugo site using hugo server -D ","date":"2020-02-04","objectID":"/2020/02/hugo-githubactions/:3:0","tags":["hugo","blog","github","automation"],"title":"Automate your GitHub Pages Deployment using Hugo and Actions","uri":"/2020/02/hugo-githubactions/"},{"categories":["Documentation"],"content":"Configure GitHub Actions You need to set up keys on the above repositories to enable publish. Generate your deploy key with the following command. ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f master -N \"\" \\# You will get 2 files: \\# master.pub (public key) \\# master (private key) 2. Add your private key as a secret with name ACTIONS_DEPLOY_KEY in the Hugo Source Repository. Navigate to Your Hugo Source Repository \u003e Settings \u003e Secrets \u003e Add new secrets and the name the secret as ACTIONS_DEPLOY_KEY and paste the contents of master file in the value. 3. Add your public key as deployment key in the GitHub pages repository Navigate to Your GitHub pages Repository \u003e Settings \u003e Deploy Key\u003e Add new deploy key and set title something you can relate to the private key (like Public Key for my site deploy) and paste the contents of master.pub file in the value. 4. Add below file .github/workflows/pages.yml location of your Hugo source code and replace username/external-repository with your GitHub username and GitHub pages repository name. 5. Commit your Hugo source code repository changes and push to GitHub. If you are using any branch other than master , you need to merge the changes to master for the workflow to start. You can change that setting in on:push:branches part of the workflow file. 6. Check the workflow in the Actions tab of your Hugo Source repository. Once your workflow completed, you can see new commits on the GitHub pages repository master branch and in few minutes the changes will appear on your GitHub page URL. ","date":"2020-02-04","objectID":"/2020/02/hugo-githubactions/:4:0","tags":["hugo","blog","github","automation"],"title":"Automate your GitHub Pages Deployment using Hugo and Actions","uri":"/2020/02/hugo-githubactions/"},{"categories":["Documentation"],"content":" In this article, I will show how you can set up a protected API endpoint using AWS Lamda, API Gateway, and automate the deployment of the stack using AWS CloudFormation. If you don’t have prior knowledge of the resources/components mentioned above, read the high-level description below. AWS Lambda AWS Lambda lets you run code without provisioning or managing servers. With Lambda, you can run code for virtually any type of application or backend service — all with zero administration. Read more Amazon API Gateway Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the “front door” for applications to access data, business logic, or functionality from your backend services. Read more AWS CloudFormation AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. Read more In the example, I have a sample inline API service deployed as Lambda and then expose the endpoint from that via the API gateway. For protecting the APIs, I will be using a managed API Key provided by AWS. There are many ways to protect an API endpoint depending on different use cases; I am not going to cover those in this article. CloudFormation Template Below is the template file, we will be using. Let’s take a look at the important definitions. ","date":"2020-02-01","objectID":"/2020/02/aws-lambda-cloudformation/:0:0","tags":["aws","lambda","cloudformation","apigateway","cloud"],"title":"Automated and Authenticated APIs stack using Lambda, API Gateway and Cloudformation (AWS)","uri":"/2020/02/aws-lambda-cloudformation/"},{"categories":["Documentation"],"content":"Lambda function I am using an inline lambda function based on NodeJS. The function responds with `Hello World!` for the API invocation. ","date":"2020-02-01","objectID":"/2020/02/aws-lambda-cloudformation/:1:0","tags":["aws","lambda","cloudformation","apigateway","cloud"],"title":"Automated and Authenticated APIs stack using Lambda, API Gateway and Cloudformation (AWS)","uri":"/2020/02/aws-lambda-cloudformation/"},{"categories":["Documentation"],"content":"API Key As mentioned before, I am using an API Key to protect the API. Below code, block define an API key, Usage Plan, and Usage Plan Key. You can see that I am adding ApiKeyRequired: true as part of the AWS::ApiGateway::Method ","date":"2020-02-01","objectID":"/2020/02/aws-lambda-cloudformation/:2:0","tags":["aws","lambda","cloudformation","apigateway","cloud"],"title":"Automated and Authenticated APIs stack using Lambda, API Gateway and Cloudformation (AWS)","uri":"/2020/02/aws-lambda-cloudformation/"},{"categories":["Documentation"],"content":"API Gateway Defining the Gateway includes many resources, including API stage and Deployment. In addition to this, you can also see a couple of IAM groups (AWS::IAM::Role) are defined on the template. One group is for the Gateway to access the Lambda function and another one for Cloud Watch log creation. I also defined a few parameters; these are optional. Create the Stack Now that you understand the resource definitions on the template file, we can proceed to create the stack. I am going to create the stack using `aws cli`. Refer this to setup your `aws cli`. You can also create the stack via AWS Console Validate the template file aws cloudformation validate-template — template-body file://stack.yaml Deploy the stack aws cloudformation deploy — stack-name lambda-api — template-file stack.yaml — capabilities CAPABILITY\\_IAM Once the stack is created, you will below in your console Waiting for changeset to be created.. Waiting for stack create/update to complete Successfully created/updated stack — lambda-api List the APIs aws apigateway get-rest-apis Get the API Id from above and construct your api url using the format https://{restapi_id}.execute-api.{region}.amazonaws.com/{stage_name}/ If you are unsure about the region, you can run aws configure get region Get your API Key Execute below command and copy value aws apigateway get-api-keys — include-value Invoke the API curl -X GET \\\\ https:/{restapi\\_id}.execute-api.{region}.amazonaws.com/v1/hello \\\\ -H ‘x-api-key: {your api key}’ For successful requests, you can see a response like below. Hello World! Conclusion You now have a protected API with a Lambda integration! As a next step, you can replace the inline Lambda function with a real-world Lambda code. You can upload your Lambda functions to the Amazon S3 bucket and use that in your template. Check the S3 example here . Good luck! ","date":"2020-02-01","objectID":"/2020/02/aws-lambda-cloudformation/:3:0","tags":["aws","lambda","cloudformation","apigateway","cloud"],"title":"Automated and Authenticated APIs stack using Lambda, API Gateway and Cloudformation (AWS)","uri":"/2020/02/aws-lambda-cloudformation/"},{"categories":null,"content":"I’m a creative and self-motivated problem solver, who like to make solutions with the help of technologies. I can take complex ideas and problems and conceptualize in a way that my audience at all levels can digest those very quickly. I thrive in collaborative functions and have the unique ability to identify and strategize opportunities. ","date":"2020-02-01","objectID":"/about/:0:0","tags":null,"title":"@me","uri":"/about/"},{"categories":["Documentation"],"content":" Spring Cloud Config is one of the best features that Spring provides as part of the framework. Spring Cloud Config allows your java application to follow Externalized configuration pattern which is must have if you are building microservices. Additionally, you can also enable the automatic config refresh in Spring Cloud Config so that all your components receive the latest configuration values when there is a change in the configuration. For automatic configuration delivery, Spring Cloud use Kafka or RabbitMQ messaging platforms. This article is going to explain how to define your own Kafka topics with Spring Cloud Config. Using default configuration, Spring Cloud Config use predefined Kafka channel springCloudBusInput and springCloudBusOutput. Also, Kafka configuration expects you to provide the zookeeper nodes using the option spring.cloud.stream.kafka.binder.zkNodes. This works well if you are using a Kafka broker that you manage or you have control. In that instance, your configuration looks like below. spring: cloud: stream: kafka: binder: zkNodes: \"\u003czookeeper-host\u003e:\u003czookeeper-port\u003e\" brokers: \"\u003cbroker-host\u003e:\u003cbroker-port\u003e\" ","date":"2018-10-17","objectID":"/2018/10/spring-cloud-config-refresh-kafka/:0:0","tags":["spring","kafka","cloud"],"title":"Spring Cloud Config Auto Refresh + Custom Kafka topic","uri":"/2018/10/spring-cloud-config-refresh-kafka/"},{"categories":["Documentation"],"content":"What if you have to mention your own Kafka topics? Above configuration won’t work if you are using a shared Kafka infrastructure which you cannot control, or you cannot create a topic dynamically on the Kafka. Luckily spring provides configuration options for you to pass custom Kafka topics and disable the auto topic creation. Below are the options you can use for supply your topic names. spring.cloud.stream.bindings.springCloudBusInput=\u003cyour\\_custom\\_input\\_topic\\_name\u003e spring.cloud.stream.bindings.springCloudBusOutput=\u003cyour\\_custom\\_output\\_topic\\_name\u003e You can disable the automatic topic creation using below option. spring.cloud.stream.kafka.binder.autoCreateTopics=false Complete configuration looks like below spring.cloud.stream: bindings: springCloudBusInput: destination: \u003cyour\\_custom\\_kafka\\_input\\_topic\u003e group: \u003coptional group id\u003espringCloudBusOutput: destination: \u003cyour\\_custom\\_kafka\\_output\\_topic\u003e kafka.binder: autoCreateTopics: false brokers: \u003cyour\\_kafka\\_broker\\_host\u003e:\u003cyour\\_kafka\\_broker\\_port\u003e ","date":"2018-10-17","objectID":"/2018/10/spring-cloud-config-refresh-kafka/:1:0","tags":["spring","kafka","cloud"],"title":"Spring Cloud Config Auto Refresh + Custom Kafka topic","uri":"/2018/10/spring-cloud-config-refresh-kafka/"},{"categories":["Documentation"],"content":"Bonus — How can you use this with mutual authentication? Spring Cloud Stream supports configuration options for mutual authentication via SSL. Here is a sample configuration. spring.cloud.stream: kafka.binder: autoCreateTopics: false brokers: \u003cyour\\_kafka\\_broker\\_host\u003e:\u003cyour\\_kafka\\_broker\\_port\u003e configuration: “\\[security.protocol\\]“: SSL “\\[ssl.enabled\\]“: true “\\[ssl.truststore.location\\]“: \u003cpath/truststore\\_file\u003e “\\[ssl.truststore.password\\]“: \u003cyour\\_truststore\\_password\u003e “\\[ssl.keystore.location\\]“: \u003cpath/keystore\\_file\u003e “\\[ssl.keystore.password\\]“: \u003cyour\\_keystore\\_password\u003e “\\[ssl.key.password\\]“: \u003cyour\\_key\\_password\u003e “\\[sasl.mechanism\\]“: PLAIN “\\[ssl.protocol\\]“: TLSv1.2 // Check this with the Cert “\\[ssl.enabled.protocols\\]“: TLSv1.2 // Check this with the Cert “\\[ssl.endpoint.identification.algorithm\\]“: HTTPS ","date":"2018-10-17","objectID":"/2018/10/spring-cloud-config-refresh-kafka/:2:0","tags":["spring","kafka","cloud"],"title":"Spring Cloud Config Auto Refresh + Custom Kafka topic","uri":"/2018/10/spring-cloud-config-refresh-kafka/"},{"categories":["Documentation"],"content":"Reference Spring Cloud Config Spring Cloud ","date":"2018-10-17","objectID":"/2018/10/spring-cloud-config-refresh-kafka/:3:0","tags":["spring","kafka","cloud"],"title":"Spring Cloud Config Auto Refresh + Custom Kafka topic","uri":"/2018/10/spring-cloud-config-refresh-kafka/"},{"categories":["Documentation"],"content":" One of the things you have to keep in mind in the cloud journey is limiting the resources (CPU and memory) for all running containers. Restrict CPU and Memory utilization across all containers is very important especially if you are running multiple containers. Docker presents an option called cgroup-parent for this purpose. You can check the official docker documentation on cgroup-parent here. Unfortunately, docker documentation doesn’t provide enough details for you start using use cgroup-parent. You can continue to read this post if you are still looking a way to apply resource limitation to your containers. First, What is cgroups (control groups)? cgroups is a feature provided by Linux kernel which allows a user to set limits on resource usage — cpu, memory, network, disk i/o. In another way, this allows specifying process level resource limitations in Linux. Read more about cgroups in redhat docs. Can I use docker –memory, –cpus options? Yes, only if you have one container per host! Yes, you can use the memory and cpu limit options mention mentioned on docker documentation. One of the things you need to keep in mind these options are per containers. For example, if you have two core cpu and you start two containers with option --cpus=\"1\", your containers are allowed to consume 100% of cpu (2 cores). You can run two containers with one cpu each using below commands. ⇒ docker run -it --rm --cpus=\"1\" busybox ⇒ docker run -it --rm --cpus=\"1\" busybox Once these containers are up, check the cpu allocated in cgroups. You will see that docker set the cpu limit you mentioned at the container start but per container. / # cat /sys/fs/cgroup/cpu/cpu.cfs\\_quota\\_us 😕So, how do we apply the restrictions consistently? Here comes the savior cgroup-parent For you to use the cgroup-parent option, first you have to define a cgroup on the host with desired limits. Different host os provides packages for creating cgroups, in this article I am going to explain how to use that using centos. I am going to do this in three steps (1) create cgroup on host for limiting cpu and memory (2) adding the cgroup-parent to the containers (3) stress test the containers. To make this easier, I have created a vagrant box with centos7 as the host and use an alpine image with stress for the containers. I also added htop on the centos to check the resource usage. You can clone repo and use vagrant up --provision to provision the box. ⇒ git clone [https://github.com/asishrs/docker-cgroup-parent.git](https://github.com/asishrs/docker-cgroup-parent.git) \u0026\u0026 docker-cgroup-parent ⇒ vagrant up --provision ","date":"2018-10-17","objectID":"/2018/10/docker-limit-cgroup/:0:0","tags":["Docker","cgroup","cloud"],"title":"Docker limit resource utilization using cgroup-parent","uri":"/2018/10/docker-limit-cgroup/"},{"categories":["Documentation"],"content":"Create cgroup on host for limiting cpu and memory I am using cgcreate command for creating a new cgroup with name c_group-limit._ / # cgcreate -g cpu:cgroup-limit Next, I am going to set cpu usage by 50% / # echo 100000 \u003e /sys/fs/cgroup/cpu/cgroup-limit/cpu.cfs\\_quota\\_us / # echo 100000 \u003e /sys/fs/cgroup/cpu/cgroup-limit/cpu.cfs\\_period\\_us You need to create one cgroup per resource type, in this case, I am trying to limit the cpu hence cpu: in my command. You can check more about cpu.cfs_quota_us and cpu.cfs_period_us here. Next, I am going to create a cgroup to limit memory to _100mb (_104857600 bytes). / # cgcreate -g memory:cgroup-limit / # echo 104857600 \u003e /sys/fs/cgroup/memory/cgroup-limit/memory.limit\\_in\\_bytes 👁️Check memory: in the command and the name cgroup-limit. Since I want to use a single cgroup to control the cpu and memory, I am using the same name for both cgroups. You can check more about memory limits here. ","date":"2018-10-17","objectID":"/2018/10/docker-limit-cgroup/:1:0","tags":["Docker","cgroup","cloud"],"title":"Docker limit resource utilization using cgroup-parent","uri":"/2018/10/docker-limit-cgroup/"},{"categories":["Documentation"],"content":"Adding the cgroup-parent to the containers You can add cgroup-parent to docker in a few different ways. The goal here is to tell all containers running on the daemon to use the same resource limits. 1. Use **--cgroup-parent** option You can pass option --cgroup-parent during the container creation. / # docker run -it --rm --cgroup-parent=/climit-cgroup/ \u003c\u003cimage-name\u003e\u003e 2. Use **cgroup_parent** option in compose file You can specify cgroup as an optional parameter for the container in the docker compose. ⚠️According to docker compose docs, this option is ignored when deploying a stack in swarm mode with a (version 3) Compose file. 3. Use **--cgroup-parent** option on dockerd You can use the --cgroup-parent option on dockerd command or specify this in the config.json I am using option #1--cgroup-parent at container creation for testing this. ","date":"2018-10-17","objectID":"/2018/10/docker-limit-cgroup/:2:0","tags":["Docker","cgroup","cloud"],"title":"Docker limit resource utilization using cgroup-parent","uri":"/2018/10/docker-limit-cgroup/"},{"categories":["Documentation"],"content":"Stress test the containers To verify the settings, I am going to use stress which allows to reserve cpu and memory so that I can see how the resource limits are working at peak. Here is the htop output before starting any containers. CPU is 0% and memory is at 115mb, we will check this once we start the containers with and without cgroups. Test the containers without cgroup limit. I am going to run this from the vagrant host as I have a specific image (asishrs/alpine-stress) I can use for stress testing. ⇒ vagrant up --provision // (Waiting...) This might take a while. ⇒ vagrant ssh // Now we are inside the vagrant box. // Here I am creating two conatiners without cgroup parent. \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -d asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -d asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s As you notice, I have started stress as part of the container. We are asking stress to use 2 cpus -c2, one IO -i 1, one worker -m 1, allocate 300M memory--vm-bytes 300Mand run for 60 seconds -t 60s. You can learn all options by adding --verbose at the end of stress command. Here is the screenshot from htop. You can see that the cpu utilization is nearly 100% and the memory is at 607mb. Test the containers with per container cgroup limit. // Here I am creating two conatiners with per container limits \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm --cpus=\"1\" --memory=\"300m\" -d asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -cpus=\"1\" --memory=\"300m\" -d asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s I have used options --cpus=\"1\"and --memory=\"300m\" during the container creation but this is applying the limit per container, that means both of the containers collectively going to consume 100% of cpu and 600mb memory. Here is the output of htop. You can see that the cpu utilization is nearly 100% and the memory is at 721mb. Now run the container with **--cgroup-parent**. // Here I am creating two conatiners with cgroup-parent. \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -d --cgroup-parent=/cgroup-limit/ asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s \\[vagrant@cgrouphost ~\\]$ sudo docker run -it --rm -d --cgroup-parent=/cgroup-limit/ asishrs/alpine-stress stress -c 2 -i 1 -m 1 --vm-bytes 300M -t 60s In the above command, I am passing the option—-cgroup-parent=/cgroup-limit/and that will restrict the overall all CPU utilization to 1 cpu and memory to 100mb. Here is the screenshot from htop. 😲 You can see that the cpu utilization is nearly 50% and the memory is at 223mb (this includes the memory for other processes on the host). Next Steps: Chekc how can you persist the cgroups so that can create atomic hosts. ","date":"2018-10-17","objectID":"/2018/10/docker-limit-cgroup/:3:0","tags":["Docker","cgroup","cloud"],"title":"Docker limit resource utilization using cgroup-parent","uri":"/2018/10/docker-limit-cgroup/"},{"categories":["Documentation"],"content":"📗Reference: https://docs.docker.com/engine/reference/commandline/dockerd/#miscellaneous-options https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01 ","date":"2018-10-17","objectID":"/2018/10/docker-limit-cgroup/:4:0","tags":["Docker","cgroup","cloud"],"title":"Docker limit resource utilization using cgroup-parent","uri":"/2018/10/docker-limit-cgroup/"}]